# 6.035 L4: Top-Down Parsing

The **shift-reduce parsers** last lecture were *bottom-up* parsers- meaning they recognize the smallest details first before the larger details, building the parse tree from the leaves up. 

This lecture discusses **recursive descent parsers**, which are *top-down* parsers that try to match the larger details first. 

## Recursive Descent Parsers
Idea: code parser as a set of mutually recursive procedures, such that the structure of program matches the structure of grammar.
* For each nonterminal, there is a procedure that attempts to parse that nonterminal.
* This procedure calls other procedures recursively according to the possible productions of that nonterminal. 

Note: this assumes that the actual input string has been put through a scanner which generates a sequence of terminal tokens. The rest of the notes refer to this sequence as the input string, not the raw unprocessed characters. 

### Basic approach
Start with the `Start` symbol of the grammar, and build a leftmost derivation using three actions:
* If the leftmost symbol is a nonterminal, choose a production and apply it- try to parse the input string according to the symbols in the production, recursively
* If the leftmost symbol is a terminal, try to match it against input token. 
* If all terminals match, then the parse is successful!

**Sentential Form** is a string of terminals and nonterminals that demonstrates how the input string is being represented by the parse tree as it is being generated by the recursive descent parser. Always starts with `Start` and becomes the input string for successful parses (this is much clearer in the slides). 

Ex: Suppose we have a very simple grammar: 
```
Start -> Int
Int -> [0-9]
String -> ".*"
```
Corresponding procedures:
```
parseStart(input: Sequence<Token>): boolean {
    return input.length == 1 and parseInt(input[0])
}
parseInt(input: Token): boolean {
    if input instanceOf IntToken
        return true
    return false
}
// ...
```

Regardless of the input string, we would start with `parseStart` and try to parse the input string as a `Start`. Then since there is only one production, we would immediately try to parse the token string as an `Int`.
* If we try to parse the input `IntToken(1)`, then the parse for the terminal `Int` would match.
* If we try to parse the input `StringToken("hello")`, then the terminal parse would reject.

We can see that the parser generates the *preorder traversal* of the parse tree- visiting parents before children, siblings from left to right. 

More detailed example in slides. 

## Production Policies
### Backtracking
So far, we have neglected to consider how the parser decides which production to select if there are multiple productions for a given nonterminal. One simple solution: backtracking!
* Treat the parse as a search problem: if the current production fails, go back to previous choice and try something different. 
* Fail when every alternative fails. 

#### Left Recursion
Consider the following grammar:
```
1: Start -> A
2: A -> A , Int
3: A -> Int
```
It is clear that the language of this grammar is a comma-separated list of one of more integers. However, rule (2) causes issues with our simple parser, since attempting to parse nonterminal `A` may require a recursive parse of `A` again! This may lead to issues with infinite recursion.
* Doesn't this mean all productions containing the LHS nonterminal break recursive descent parsers? Not quite- the key is that `A` in (2) appears as the *leftmost symbol*. If instead, for example, a terminal appeared as the first symbol, there wouldn't be this issue because the parser would make progress through the string for every recursive parse of `A`. Hence the name, **left recursion**. In fact, this idea hints at how left recursion can be fixed.

A recurring theme for when your grammar has an issue is to simply hack the grammar to eliminate the issue (ex. precedence). Here, we introduce a new nonterminal to turn rule (2) into an equivalent set of rules with a terminal as the leftmost symbol:
```
Start -> A
A -> Int A'
A' -> , Int A'
A' -> e // empty string AKA epsilon
``` 

### Predictive Parsing
Backtracking is quite inefficient. As a human reading (and mentally parsing), for example an arithmetic expression with parentheses, we don't just look at one symbol at a time, but multiple. 

This is the idea behind **predictive parsing**- we take into account the next few tokens to decide which production to apply. 
* The number of tokens that the parser takes into account is appropriately called the **lookahead**. We use 1 token of lookahead for simplicity (and as it turns out, 1 token of lookahead is good enough for most practical grammars). 

Ex. Suppose we have the following grammar:
```
...
A -> ( A )
A -> Int
...
```
If we were parsing `A`, we would simply check whether the next token is an open paren `(` or an integer and then apply the corresponding production.

#### Left Factoring
What if there are multiple productions that have the same terminal as the leftmost symbol?

Solution: Hack the grammar (specifically, **left factor** it) so that each production has a unique leftmost terminal. Ex:
```
...
A -> Int + A
A -> Int - A
...
```
After left factoring:
```
A -> Int A'
A' -> + A
A' -> - A
```
The key idea is to merge the rules, then create a new nonterminal for the different symbols towards the right of the production. This works even for rules with multiple identical leftmost terminals. It is exactly the same as factoring in arithmetic: $ab + ac = a(b+c)$- in fact this is exactly what happens in the abstract algebra representation of the grammar!

#### First Symbol Derivation
What if a nonterminal is the leftmost symbol in a production?

The key idea is to choose which production to apply by deriving which set of terminals can possibly occur *first* in each production- so that we may match the next token to this first terminal.

For each production `B` we define a set $\text{First}(B)$ as the *minimal* set of terminals that can appear as the first symbol in a derivation starting at `B`.

We may precompute these sets based on the grammar using a fixed-point algorithm with the following transformations (example in slides).
1. Obviously, for any terminal `a`, $\text{First}(a) = \{a\}$.
2. For any symbol `S`, $\text{First}(S) \subseteq \text{First}(S\beta)$ for any sequence $\beta$ of symbols. What this says is that if a symbol appears first in a production, then the first set for that symbol must be included in the first set for the production.
3. For any nonterminal `N`, if `N` derives $\epsilon$ (empty string), then $\text{First}(\beta) \subseteq \text{First}(N\beta)$. What this says is that the first set should "fall through" `N` since it may derive nothing.
4. If `N` derives $S\;\beta$, then $\text{First}(S \beta) \subseteq \text{First}(N)$.

## Hand-Coded Parsers vs Generated Parsers
While hand-written recursive descent parsers are more work, they are generally the best solution for complicated grammars that parser generators may give up on. Once written, they are also much easier to optimize for performance and integrate with the rest of the compiler. 

Example of hand-coded parser in slides for a simple grammar. 
